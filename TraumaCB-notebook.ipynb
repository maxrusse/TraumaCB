{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f68adb6-b6de-47ea-9421-4947decc68e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Documents loaded.\n",
      "Building Vector Store Index...\n",
      "Vector Store Index created.\n",
      "Persisting index to directory: INDEX\n",
      "Index persisted successfully.\n"
     ]
    }
   ],
   "source": [
    "### Code written based on LLAMA_INDEX v0.8 and OPENAI shortly after release of GPT4 Turbo\n",
    "### for using with recent Versions of OpenAI and LLAMA_Index - Major Code-Changes are to be expected!\n",
    "#Version used for this code:\n",
    "# PyPDF2 (PyPDF2): 3.0.1 [Pip]\n",
    "# langchain (langchain): 0.0.332 [Pip]\n",
    "# llama-index (llama_index): 0.8.65 [Pip]\n",
    "# nest_asyncio (nest_asyncio): 1.5.8 [Pip]\n",
    "# openai (openai): 1.2.0 [Pip]\n",
    "# pypdf (pypdf): 3.15.1 [Pip]\n",
    "\n",
    "\n",
    "# ### Cell 1: Building and Persisting the Vector Store Index\n",
    "\n",
    "# Import Necessary Libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import nest_asyncio  \n",
    "import pandas as pd\n",
    "import pypdf\n",
    "import PyPDF2\n",
    "\n",
    "# Importing classes and functions from llama_index library for indexing and querying\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    LLMPredictor,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "# Importing ChatOpenAI from langchain for language model interactions\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import openai  # OpenAI API client\n",
    "\n",
    "# Apply Nest Asyncio\n",
    "# This allows the Jupyter notebook to handle asynchronous operations properly.\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration and Environment Setup\n",
    "\n",
    "# Security Note: Ensure API keys are managed securely.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'sk-...'  # Replace with your actual API key securely\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Fetch the API key from environment variables\n",
    "\n",
    "# Logging Configuration\n",
    "# Uncomment the following lines to enable logging for debugging purposes.\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Define Constants\n",
    "INPUT_DIRECTORY = 'TRAUMA'  # Directory containing your input documents (e.g., PDFs)\n",
    "PERSIST_DIR = \"INDEX\"      # Directory where the index will be saved for persistence\n",
    "\n",
    "# LLM (Language Model) Configuration Parameters\n",
    "# Embedding model ist default to text-embedding-ada-002\n",
    "LLM_MODEL_NAME = \"gpt-4-1106-preview\"  # Choose between models like \"gpt-4\" or \"gpt-4-1106-preview\" \n",
    "LLM_TEMPERATURE = 0.4              # Controls the randomness of the model's output\n",
    "CONTEXT_WINDOW_SIZE = 48192         # Feasible amount tokens the model can handle in context\n",
    "CHUNK_SIZE = 1024                   # Size of text chunks for processing\n",
    "EMBED_BATCH_SIZE = 150              # Batch size for embedding generation\n",
    "\n",
    "# Function to Attach Filename Metadata to Documents\n",
    "def attach_filename_metadata(filename):\n",
    "    \"\"\"\n",
    "    Attaches the filename as metadata to each document.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the filename metadata.\n",
    "    \"\"\"\n",
    "    return {'file_name': filename}\n",
    "\n",
    "# Initialize LLMPredictor with ChatOpenAI\n",
    "llm_predictor = LLMPredictor(\n",
    "    llm=ChatOpenAI(temperature=LLM_TEMPERATURE, model_name=LLM_MODEL_NAME)\n",
    ")\n",
    "\n",
    "# Create a Sentence Window Node Parser with Default Settings\n",
    "# This parser splits documents into overlapping sentences (windows) for better context handling.\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=5,  # Number of sentences surrounding the embedded sentence in each window\n",
    "    window_metadata_key=\"window\",  # Metadata key for window information\n",
    "    original_text_metadata_key=\"original_text\",  # Metadata key for original text\n",
    ")\n",
    "\n",
    "# Set Up the Service Context\n",
    "# The ServiceContext combines the LLM predictor, embedding model, and node parser.\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    "    embed_model=OpenAIEmbedding(embed_batch_size=EMBED_BATCH_SIZE),\n",
    "    node_parser=node_parser,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    context_window=CONTEXT_WINDOW_SIZE\n",
    ")\n",
    "\n",
    "# Load Documents and Build Index\n",
    "\n",
    "print('Loading documents...')\n",
    "# Reads documents from the specified input directory and attaches filename metadata\n",
    "documents = SimpleDirectoryReader(INPUT_DIRECTORY, file_metadata=attach_filename_metadata).load_data()\n",
    "print('Documents loaded.')\n",
    "\n",
    "print('Building Vector Store Index...')\n",
    "# Creates a VectorStoreIndex from the loaded documents using the service context\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "print('Vector Store Index created.')\n",
    "\n",
    "print(f'Persisting index to directory: {PERSIST_DIR}')\n",
    "# Saves the index to the specified persistence directory for later use\n",
    "index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "print('Index persisted successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36bf141-d138-4df6-9aa2-f7ffa27d3732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Response (Primary Diagnosis):\n",
      "- Primary diagnosis: Isler type II sacral fracture with limited lumbosacral instability\n",
      "----------------------------------------------------------------\n",
      "Second Response (Classification and Grading):\n",
      "- Primary diagnosis: Isler type II sacral fracture with limited lumbosacral instability\\n\\n---------------------\\n\\n- Radiological classification system for this diagnosis: Isler Classification of Lumbosacral Instability\\n\\n---------------------\\n\\n- Grading in the classification system for this diagnosis: Type II\\n\\n---------------------\\n\\nExplanation: The Isler Classification of Lumbosacral Instability is used to assess the integrity and stability of the lumbosacral junction in patients with vertical sacral fractures that extend to involve the L5-S1 facets. It is particularly relevant when Denis zone 2 fractures propagate superiorly. In the case described, the CT scan shows a vertically oriented transforaminal fracture through the left sacrum with upward extension involving the L5-S1 facets and significant widening of the facet joint. However, there is no dissociation of the superior sacral facet from the medial sacrum, which is consistent with an Isler type II injury. This type of injury indicates limited lumbosacral instability, as the continuity between the facet and medial sacrum is preserved, but there is facet joint widening that may lead to chronic pain or facet arthropathy. The primary diagnosis is based on the imaging findings and the Isler classification system, which guides the grading of the injury as type II.\n",
      "Source Files and Pages:\n",
      "Dreizin und Smith - 2022 - CT of Sacral Fractures Classification Systems and.pdf Page: (1978, 1979, 1981, 1982, 1984, 1985, 1986, 1987, 1988, 1990, 1991)\n"
     ]
    }
   ],
   "source": [
    "# ### Cell 2: Querying the Vector Store Index \n",
    "\n",
    "# Import Necessary Libraries for Application\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import pypdf\n",
    "import PyPDF2\n",
    "\n",
    "# Importing additional classes and functions from llama_index for querying and post-processing\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    LLMPredictor,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    QuestionAnswerPrompt,\n",
    "    RefinePrompt\n",
    ")\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.query_engine.transform_query_engine import TransformQueryEngine\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import openai  # OpenAI API client\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Apply nest_asyncio again if not already applied\n",
    "\n",
    "# Configuration and Environment Setup\n",
    "\n",
    "# Security Note: Ensure API keys are managed securely.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = 'sk-...'  # Replace with your actual API key securely\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Fetch the API key from environment variables\n",
    "\n",
    "# Logging Configuration\n",
    "# Uncomment the following lines to enable logging for debugging purposes.\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Define Constants\n",
    "PERSIST_DIR = \"INDEX\"  # Directory where the index is persisted\n",
    "\n",
    "# LLM Configuration Parameters for Querying\n",
    "# Embedding model is default to text-embedding-ada-002\n",
    "QUERY_MODEL = \"gpt-4-1106-preview\"    # Version used in paper, choose newer models if desired\n",
    "QUERY_TEMPERATURE = 0.4               # Controls the randomness of the model's output\n",
    "QUERY_CONTEXT_WINDOW = 48192          # Feasible amount of tokens the model can handle in context for queries\n",
    "QUERY_NUM_OUTPUT_TOKENS = 1024        # Number of tokens the model should generate in response\n",
    "SIMILARITY_TOP_K = 20                 # Number of top similar documents to retrieve\n",
    "RESPONSE_MODE = \"compact\"             # Mode for the response formatting\n",
    "\n",
    "\n",
    "# Function to Load the Existing Index from Storage\n",
    "def load_existing_index(persist_dir):\n",
    "    \"\"\"\n",
    "    Loads the existing index from the specified persistence directory.\n",
    "\n",
    "    Args:\n",
    "        persist_dir (str): The directory where the index is persisted.\n",
    "\n",
    "    Returns:\n",
    "        VectorStoreIndex: The loaded index.\n",
    "    \"\"\"\n",
    "    logging.info(f'Loading index from storage directory: {persist_dir}')\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    logging.info('Index loaded successfully.')\n",
    "    return index\n",
    "\n",
    "# Define Prompt Templates\n",
    "\n",
    "## 1st Step\n",
    "\n",
    "# --- Helper Functions ---\n",
    "ENGLISH_QA_PROMPT_TMPL1 = (\n",
    "    \"We have provided scientific context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please evaluate the following image finding: {query_str}\\n\"   \n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Please follow the structure below for your response:\\n\"\n",
    "    \"- Primary diagnosis: (Main pathologic finding as working diagnosis, no explanation)\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    ")\n",
    "\n",
    "ENGLISH_QA_PROMPT1 = QuestionAnswerPrompt(ENGLISH_QA_PROMPT_TMPL1)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "ENGLISH_REFINE_PROMPT_TMPL1 = (\n",
    "    \"The original case file with the initial image finding is as follows:\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{query_str}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"The initial evaluation of the image finding provided is:\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{existing_answer}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Additional context information is provided below to potentially refine the evaluation of the image finding.\\n\"\n",
    "    \"If necessary, refine the evaluation of the image finding based on the new context.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the new context, please refine the evaluation of the image finding. The evaluation might change or be adopted based on new context.\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Please follow the structure below for your response:\\n\"\n",
    "    \"- Primary diagnosis: (Refined pathologic finding as working diagnosis, no explanation)\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"If the new context is not useful, repeat exactly the original answer.\\n\"\n",
    ")\n",
    "\n",
    "ENGLISH_REFINE_PROMPT1 = RefinePrompt(ENGLISH_REFINE_PROMPT_TMPL1)\n",
    "\n",
    "## 2nd Step\n",
    "\n",
    "# --- Helper Functions ---\n",
    "ENGLISH_QA_PROMPT_TMPL2 = (\n",
    "    \"We have provided scientific context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this information, please evaluate the following image finding and suggested primary diagnosis: {query_str}\\n\"   \n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Please follow the structure below for your response:\\n\"\n",
    "    \"- Primary diagnosis: (Main pathologic finding as working diagnosis, no explanation)\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"- Radiological classification system for this diagnosis (Please state only one classification! If there is no appropriate classification, specify this):\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"- Grading in the classification system for this diagnosis (Please state only one grade! If there is no classification system, specify this):\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Explanation: Provide a concise yet comprehensive and self-contained explanation summarizing the key points of the main diagnosis, classification system and grading.\"\n",
    "    \"\\n---------------------\\n\"\n",
    ")\n",
    "\n",
    "ENGLISH_QA_PROMPT2 = QuestionAnswerPrompt(ENGLISH_QA_PROMPT_TMPL2)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "ENGLISH_REFINE_PROMPT_TMPL2 = (\n",
    "    \"The original case file with the initial image finding and suggested primary diagnosis is as follows:\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{query_str}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"The initial evaluation of the image finding provided is:\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{existing_answer}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Additional context information is provided below to potentially refine the evaluation of the image finding.\\n\"\n",
    "    \"If necessary, refine the evaluation of the image finding based on the new context.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given the new context, please refine the evaluation of the image finding. The evaluation might change or be adopted based on new context.\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Please follow the structure below for your response:\\n\"\n",
    "    \"- Primary diagnosis: (Refined pathologic finding as working diagnosis, no explanation)\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"- Radiological classification system for this diagnosis (Please state only one classification! If there is no appropriate classification, specify this):\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"- Grading in the classification system for this diagnosis (Please state only one grade! If there is no classification system, specify this):\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Explanation: Provide a concise yet comprehensive and self-contained explanation summarizing the key points of the refined main diagnosis, radiological classification system and grading (if applicable).\\n\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"If the new context is not useful, repeat exactly the original answer.\\n\"\n",
    ")\n",
    "\n",
    "ENGLISH_REFINE_PROMPT2 = RefinePrompt(ENGLISH_REFINE_PROMPT_TMPL2)\n",
    "\n",
    "def get_filenames(response):\n",
    "    \"\"\"\n",
    "    Extracts and summarizes filenames and their corresponding pages from the query response.\n",
    "\n",
    "    Args:\n",
    "        response (Response): The response object from the query.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string summarizing source files and pages.\n",
    "    \"\"\"\n",
    "    filenames_pages = dict()\n",
    "    for doc_id, metadata in response.metadata.items():\n",
    "        filename_label = metadata.get(\"file_name\")\n",
    "        page_label = metadata.get(\"page_label\")\n",
    "        if filename_label is not None:\n",
    "            filename_label = os.path.basename(filename_label)\n",
    "            if filename_label not in filenames_pages:\n",
    "                filenames_pages[filename_label] = set()\n",
    "            if page_label is not None:\n",
    "                filenames_pages[filename_label].add(f\"{page_label}\")\n",
    "    summary = []\n",
    "    for filename, pages in filenames_pages.items():\n",
    "        # Sort pages numerically if possible\n",
    "        sorted_pages = sorted(pages, key=lambda x: (not x.isdigit(), int(x) if x.isdigit() else x))\n",
    "        summary.append(f\"{filename} Page: ({', '.join(sorted_pages)})\" if pages else filename)\n",
    "    return \", \".join(summary)\n",
    "\n",
    "# Set up the LLMPredictor and ServiceContext\n",
    "llm_predictor = LLMPredictor(\n",
    "    llm=ChatOpenAI(\n",
    "        temperature=QUERY_TEMPERATURE, \n",
    "        model_name=QUERY_MODEL, \n",
    "    )\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor, \n",
    "    chunk_size=QUERY_NUM_OUTPUT_TOKENS, \n",
    "    context_window=QUERY_CONTEXT_WINDOW\n",
    ")\n",
    "\n",
    "# Load the Existing Index from Storage ## might take some time, disable when already loaded once\n",
    "index = load_existing_index(PERSIST_DIR)\n",
    "\n",
    "# Initialize Query Engines for Both Steps\n",
    "query_engine1 = index.as_query_engine(\n",
    "    service_context=service_context,\n",
    "    similarity_top_k=SIMILARITY_TOP_K,\n",
    "    response_mode=RESPONSE_MODE,\n",
    "    text_qa_template=ENGLISH_QA_PROMPT1,\n",
    "    refine_template=ENGLISH_REFINE_PROMPT1,\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    ")\n",
    "\n",
    "query_engine2 = index.as_query_engine(\n",
    "    service_context=service_context,\n",
    "    similarity_top_k=SIMILARITY_TOP_K,\n",
    "    response_mode=RESPONSE_MODE,\n",
    "    text_qa_template=ENGLISH_QA_PROMPT2,\n",
    "    refine_template=ENGLISH_REFINE_PROMPT2,\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Define the Case Description\n",
    "casedesc = '''\n",
    "CT scan of the lumbosacral spine reveals a vertically oriented transforaminal fracture through the left sacrum. The fracture extends upward, involving the L5-S1 facets and causing significant widening of the facet joint, but no dissociation of the superior sacral facet from the medial sacrum is seen.\n",
    "'''\n",
    "\n",
    "# Prepare Input Text for the First Query\n",
    "input_text = f'{casedesc}'\n",
    "\n",
    "# Query the Index and Get the First Response\n",
    "response1 = query_engine1.query(input_text)\n",
    "\n",
    "# Process the First Response\n",
    "output_accGPT1 = response1.response.replace('\\n\\n\\n', '\\\\n\\\\n\\\\n').replace('\\n\\n', '\\\\n\\\\n').replace('\\n', '\\\\n')\n",
    "\n",
    "# Prepare Input Text for the Second Query\n",
    "input_text2 = f'{casedesc} \\n---------------------\\n {output_accGPT1} Next step: Classification and Grading'\n",
    "\n",
    "# Query the Index and Get the Second Response\n",
    "response2 = query_engine2.query(input_text2)\n",
    "\n",
    "# Process the Second Response\n",
    "output_accGPT2 = response2.response.replace('\\n\\n\\n', '\\\\n\\\\n\\\\n').replace('\\n\\n', '\\\\n\\\\n').replace('\\n', '\\\\n')\n",
    "\n",
    "# Get the Corresponding Filenames\n",
    "filenames = get_filenames(response2)\n",
    "\n",
    "# Display the Results\n",
    "print(\"First Response (Primary Diagnosis):\")\n",
    "print(output_accGPT1)\n",
    "print('----------------------------------------------------------------')\n",
    "print(\"Second Response (Classification and Grading):\")\n",
    "print(output_accGPT2)\n",
    "print(\"Source Files and Pages:\")\n",
    "print(filenames)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_old",
   "language": "python",
   "name": "openai_old"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
